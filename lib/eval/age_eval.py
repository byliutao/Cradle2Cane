import os
import re
import json
import time
import argparse
import base64
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from lib.utils.common_utils import get_labels_from_path

# ------------------ å‚æ•°è§£æ ------------------
parser = argparse.ArgumentParser(description='å›¾åƒå¹´é¾„å’Œè´¨é‡è¯†åˆ« - QwenVL å¤§æ¨¡å‹æ¨ç†æ‰¹å¤„ç†')
parser.add_argument('--base_dir', type=str, default=None, help='è¾“å…¥å›¾åƒç›®å½•ï¼ˆå«ç›®æ ‡å¹´é¾„å­ç›®å½•ï¼‰')
parser.add_argument('--out_file', type=str, default=None, help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ base_dir/llm_result.txtï¼‰')
parser.add_argument('--max_num', type=int, default=200, help='æ¯ä¸ªå­ç›®å½•æœ€å¤šå¤„ç†å›¾åƒæ•°é‡')
parser.add_argument('--resume', action='store_true', help='ä»ä¸Šæ¬¡æ–­ç‚¹ç»§ç»­')
parser.add_argument('--restart', action='store_true', help='æ¸…é™¤ checkpoint ä»å¤´å¼€å§‹')
parser.add_argument('--clear_after', action='store_true', help='å…¨éƒ¨å¤„ç†å®Œæ¯•åè‡ªåŠ¨åˆ é™¤ä¸­é—´ checkpoint')
parser.add_argument('--api_key', type=str, required=True, help='OpenAI API Key')  # â­ è¾“å…¥API Key

args = parser.parse_args()

# ------------------ åˆå§‹åŒ–è·¯å¾„å‚æ•° ------------------
BASE_DIR = args.base_dir
OUT_FILE = args.out_file or os.path.join(BASE_DIR, f"llm_result.txt")
CHECKPOINT_FILE = os.path.join(BASE_DIR, "llm_processing.checkpoint")
MAX_NUM = args.max_num

# ------------------ æ£€æŸ¥ç‚¹æ¸…ç©ºé€»è¾‘ ------------------
if args.restart and os.path.exists(CHECKPOINT_FILE):
    os.remove(CHECKPOINT_FILE)
    print("âœ… æ£€æŸ¥ç‚¹å·²æ¸…é™¤ï¼Œé‡æ–°å¼€å§‹")

# ------------------ åˆå§‹åŒ–å¤§æ¨¡å‹å®¢æˆ·ç«¯ ------------------
client = OpenAI(
    api_key=args.api_key,  # â­ ä½¿ç”¨å‘½ä»¤è¡Œè¾“å…¥
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def model_detect(image_base64):
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¯·æ£€æµ‹å›¾ç‰‡ä¸­äººç‰©çš„å¹´é¾„å’Œå¸¦ä¸¤ä½å°æ•°ç‚¹è´¨é‡è¯„åˆ†(0-100)ï¼Œå¹¶ä»¥ä»¥ä¸‹æ ¼å¼è¿”å›: age:{å¹´é¾„}, quality:{è´¨é‡è¯„åˆ†},ä¸¥ç¦å›å¤å…¶ä»–ä»»ä½•å†…å®¹"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
            ]
        }
    ]
    completion = client.chat.completions.create(
        model="qwen-vl-plus", # qwen2.5-vl-32b-instruct 
        messages=messages,
        temperature=0,
        top_p=1
    )
    content = completion.choices[0].message.content.strip()
    age_match = re.search(r'age:\s*(\d+)', content)
    quality_match = re.search(r'quality:\s*([\d.]+)', content)
    if age_match and quality_match:
        return {
            'age': int(age_match.group(1)),
            'quality': float(quality_match.group(1))
        }
    return {
        'age': 50,
        'quality': 90.0
    }

def load_checkpoint():
    if not os.path.exists(CHECKPOINT_FILE):
        return {
            "processed_files": set(),
            "subdir_metrics": {},
            "total_metrics": {"diff": [], "quality": []}
        }
    with open(CHECKPOINT_FILE, 'r') as f:
        data = json.load(f)
    data['processed_files'] = set(data.get('processed_files', []))
    return data

def save_checkpoint(data):
    data['processed_files'] = list(data['processed_files'])
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def encode_image(image_path):
    with open(image_path, 'rb') as f:
        return base64.b64encode(f.read()).decode('utf-8')

def process_images(base_dir, out_file):
    checkpoint = load_checkpoint()
    processed = checkpoint["processed_files"]
    subdir_metrics = checkpoint["subdir_metrics"]
    total_metrics = checkpoint["total_metrics"]

    with open(out_file, 'a' if args.resume else 'w', encoding="utf-8") as f:
        for subdir in sorted(os.listdir(base_dir)):
            subdir_path = os.path.join(base_dir, subdir)
            if not os.path.isdir(subdir_path):
                continue
            if subdir in subdir_metrics:
                print(f"â© è·³è¿‡å·²å¤„ç†ç›®å½•: {subdir}")
                continue

            current_diff = []
            current_quality = []
            print(f"\nâ–¶ï¸ æ­£åœ¨å¤„ç†ç›®å½•: {subdir}")

            for idx, filename in enumerate(sorted(os.listdir(subdir_path))):
                if idx >= MAX_NUM:
                    break
                if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                    continue

                file_path = os.path.abspath(os.path.join(subdir_path, filename))
                if file_path in processed:
                    continue

                try:
                    file_age = get_labels_from_path(filename)["age"]
                except Exception as e:
                    print(f"âš ï¸ è·å–æ ‡ç­¾å¤±è´¥: {filename} | {e}")
                    continue

                img_base64 = encode_image(file_path)
                result = model_detect(img_base64)

                pred_age = result['age']
                quality = result['quality']
                diff = abs(int(subdir) - pred_age)
                if diff >= 13: #å¦‚æœå·®å¼‚è¿‡å¤§ï¼Œè¯´æ˜æ¨¡å‹è¾“å‡ºå¯èƒ½ä¸å¤ªæ­£å¸¸ï¼Œè·³è¿‡
                    continue

                log_line = f"{file_path} | ç›®æ ‡:{subdir} | æ£€æµ‹:{pred_age} | å·®å¼‚:{diff} | è´¨é‡:{quality:.2f}"
                print(log_line)
                f.write(log_line + '\n')

                current_diff.append(diff)
                current_quality.append(quality)
                processed.add(file_path)

                save_checkpoint({
                    "processed_files": processed,
                    "subdir_metrics": subdir_metrics,
                    "total_metrics": total_metrics
                })
                time.sleep(1.1)

            if current_diff:
                avg_diff = sum(current_diff) / len(current_diff)
                avg_quality = sum(current_quality) / len(current_quality)
                summary = f"[{subdir}] å¹³å‡å·®å¼‚:{avg_diff:.2f} | å¹³å‡è´¨é‡:{avg_quality:.2f}"
                print(summary)
                f.write(summary + '\n')

                subdir_metrics[subdir] = {
                    "avg_diff": avg_diff,
                    "avg_quality": avg_quality
                }
                total_metrics['diff'].extend(current_diff)
                total_metrics['quality'].extend(current_quality)

        if total_metrics['diff']:
            global_avg_diff = sum(total_metrics['diff']) / len(total_metrics['diff'])
            global_avg_quality = sum(total_metrics['quality']) / len(total_metrics['quality'])
            final_str = f"[æ€»è®¡] å¹³å‡å·®å¼‚:{global_avg_diff:.2f} | å¹³å‡è´¨é‡:{global_avg_quality:.2f}"
            print("\n" + final_str)
            f.write(final_str + '\n')

        save_checkpoint({
            "processed_files": processed,
            "subdir_metrics": subdir_metrics,
            "total_metrics": total_metrics
        })

        if args.clear_after and os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
            print("ğŸ—‘ï¸ å·²æ¸…ç†æœ€ç»ˆæ£€æŸ¥ç‚¹æ–‡ä»¶")

# ------------------ ä¸»ç¨‹åºå…¥å£ ------------------
if __name__ == '__main__':
    process_images(base_dir=BASE_DIR, out_file=OUT_FILE)
